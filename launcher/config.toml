[model]
device = "cuda"
mlp_ratio = 4
d_model = 4096
n_heads = 32
rope_theta = 10000.0
max_sequence_length = 1024
vocab_size = 126464
n_layers = 32
mlp_hidden_size = 12288
ckpt_path = "model.safetensors"
repo_id = "trixyL/LLaDA-8B-Instruct-merged"
dtype = "float16"

[tokenizer]
vocab_path = "vocab.json"
merges_path = "merges.txt"
special_tokens = "special_tokens.json"
repo_id = "trixyL/LLaDA-8B-Instruct-merged"

[router]
host = "localhost"
port = 3001
binary = "../router/target/debug/router"

[worker]
sock = "/tmp/worker.sock"
command = "uv"
args = ["run", "--project", "../worker", "python", "../worker/server.py"]
